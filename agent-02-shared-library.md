# Agent 2: Shared Library

## ğŸ­ Rola
**Backend Library Developer**

## ğŸ¯ Cel
Stworzenie wspÃ³lnej biblioteki Python uÅ¼ywanej przez wszystkie mikroserwisy

## ğŸ“– Kontekst

Mikroserwisy (Collector, Refinery, Finance) potrzebujÄ… wspÃ³Å‚dzielonego kodu:
- **Komunikacja** - jednolity sposÃ³b wysyÅ‚ania zadaÅ„ przez Redis
- **Typy danych** - wspÃ³lne modele Pydantic dla wszystkich serwisÃ³w
- **Konfiguracja** - centralne zarzÄ…dzanie zmiennymi Å›rodowiskowymi
- **Logowanie** - strukturalne logi w formacie JSON

Tworzymy pakiet Python instalowany przez `pip install -e ./shared`, ktÃ³ry bÄ™dzie importowany w kaÅ¼dym mikroserwisie.

## âœ… Zadania

### 1. StwÃ³rz StrukturÄ™ Pakietu

```
shared/
â”œâ”€â”€ setup.py                 # Definicja pakietu
â”œâ”€â”€ requirements.txt         # ZaleÅ¼noÅ›ci
â”œâ”€â”€ README.md               # Dokumentacja biblioteki
â”œâ”€â”€ tests/                  # Testy jednostkowe
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ test_messaging.py
â”‚   â”œâ”€â”€ test_types.py
â”‚   â””â”€â”€ test_config.py
â””â”€â”€ shared/                 # GÅ‚Ã³wny pakiet
    â”œâ”€â”€ __init__.py
    â”œâ”€â”€ messaging.py        # Redis client
    â”œâ”€â”€ types.py           # Pydantic models
    â”œâ”€â”€ config.py          # Environment config
    â”œâ”€â”€ logging.py         # Structured logging
    â””â”€â”€ utils.py           # Helper functions
```

### 2. Plik `setup.py`

```python
from setuptools import setup, find_packages

setup(
    name="obsidian-brain-shared",
    version="2.0.0",
    description="Shared library for Obsidian Brain microservices",
    author="Your Name",
    packages=find_packages(),
    install_requires=[
        "redis>=5.0.0",
        "pydantic>=2.0.0",
        "pydantic-settings>=2.0.0",
        "python-dotenv>=1.0.0",
        "structlog>=24.0.0",
    ],
    python_requires=">=3.10",
    extras_require={
        "dev": [
            "pytest>=7.0.0",
            "pytest-asyncio>=0.21.0",
            "black>=23.0.0",
            "ruff>=0.1.0",
        ]
    },
)
```

### 3. Plik `requirements.txt`

```txt
redis>=5.0.0
pydantic>=2.0.0
pydantic-settings>=2.0.0
python-dotenv>=1.0.0
structlog>=24.0.0
```

### 4. ModuÅ‚ `shared/__init__.py`

```python
"""
Obsidian Brain Shared Library
WspÃ³lny kod dla wszystkich mikroserwisÃ³w
"""

__version__ = "2.0.0"

from .messaging import RedisClient, TaskQueue
from .types import (
    ArticleTask,
    YoutubeTask,
    ReceiptTask,
    ProcessedNote,
    TaskStatus,
)
from .config import Settings, get_settings
from .logging import setup_logging, get_logger

__all__ = [
    "RedisClient",
    "TaskQueue",
    "ArticleTask",
    "YoutubeTask",
    "ReceiptTask",
    "ProcessedNote",
    "TaskStatus",
    "Settings",
    "get_settings",
    "setup_logging",
    "get_logger",
]
```

### 5. ModuÅ‚ `shared/messaging.py` (Klient Redis)

```python
"""
Redis messaging dla komunikacji miÄ™dzy serwisami
"""
import json
import redis
from typing import Callable, Any, Optional
from datetime import datetime
import structlog

logger = structlog.get_logger()


class RedisClient:
    """
    Wrapper na Redis do zarzÄ…dzania kolejkami zadaÅ„
    """
    
    def __init__(
        self,
        host: str = "redis",
        port: int = 6379,
        db: int = 0,
        decode_responses: bool = True
    ):
        """
        Args:
            host: Redis host
            port: Redis port
            db: Database number
            decode_responses: Czy dekodowaÄ‡ odpowiedzi jako string
        """
        self.client = redis.Redis(
            host=host,
            port=port,
            db=db,
            decode_responses=decode_responses,
            socket_connect_timeout=5,
            socket_timeout=5,
            retry_on_timeout=True,
            health_check_interval=30,
        )
        logger.info("redis_connected", host=host, port=port, db=db)
    
    def ping(self) -> bool:
        """SprawdÅº czy Redis odpowiada"""
        try:
            return self.client.ping()
        except redis.ConnectionError:
            logger.error("redis_connection_failed")
            return False
    
    def publish_task(
        self,
        queue_name: str,
        payload: dict,
        priority: int = 0
    ) -> bool:
        """
        WyÅ›lij zadanie do kolejki
        
        Args:
            queue_name: Nazwa kolejki (np. 'queue:refinery')
            payload: Dane zadania (dict)
            priority: Priorytet (wyÅ¼szy = waÅ¼niejsze)
        
        Returns:
            True jeÅ›li sukces
        """
        try:
            # Dodaj metadata
            task = {
                **payload,
                "metadata": {
                    "enqueued_at": datetime.utcnow().isoformat(),
                    "priority": priority,
                }
            }
            
            # LPUSH dodaje na poczÄ…tek listy (FIFO z RPOP)
            self.client.lpush(queue_name, json.dumps(task))
            
            logger.info(
                "task_published",
                queue=queue_name,
                task_id=payload.get("id"),
                priority=priority
            )
            return True
            
        except Exception as e:
            logger.error(
                "task_publish_failed",
                queue=queue_name,
                error=str(e)
            )
            return False
    
    def listen_to_queue(
        self,
        queue_name: str,
        callback: Callable[[dict], None],
        timeout: int = 0
    ) -> None:
        """
        NasÅ‚uchuj na kolejce i wywoÅ‚uj callback dla kaÅ¼dego zadania
        
        Args:
            queue_name: Nazwa kolejki
            callback: Funkcja przetwarzajÄ…ca zadanie callback(task_data)
            timeout: Timeout w sekundach (0 = blokujÄ…ce czekanie)
        
        PrzykÅ‚ad:
            def process_task(task):
                print(f"Processing: {task}")
            
            client.listen_to_queue("queue:refinery", process_task)
        """
        logger.info("queue_listener_started", queue=queue_name)
        
        while True:
            try:
                # BRPOP - blokujÄ…ce pobranie z koÅ„ca listy
                result = self.client.brpop(queue_name, timeout=timeout or 0)
                
                if result:
                    _, task_json = result
                    task = json.loads(task_json)
                    
                    logger.info(
                        "task_received",
                        queue=queue_name,
                        task_id=task.get("id")
                    )
                    
                    # WywoÅ‚aj callback
                    callback(task)
                    
            except json.JSONDecodeError as e:
                logger.error("task_json_decode_error", error=str(e))
            except KeyboardInterrupt:
                logger.info("queue_listener_stopped", queue=queue_name)
                break
            except Exception as e:
                logger.error(
                    "queue_listener_error",
                    queue=queue_name,
                    error=str(e)
                )
    
    def get_queue_length(self, queue_name: str) -> int:
        """Ile zadaÅ„ w kolejce"""
        return self.client.llen(queue_name)
    
    def clear_queue(self, queue_name: str) -> bool:
        """WyczyÅ›Ä‡ wszystkie zadania z kolejki"""
        try:
            self.client.delete(queue_name)
            logger.warning("queue_cleared", queue=queue_name)
            return True
        except Exception as e:
            logger.error("queue_clear_failed", queue=queue_name, error=str(e))
            return False


class TaskQueue:
    """
    Wysokopoziomowy wrapper - uproszczone API dla standardowych operacji
    """
    
    # Standardowe nazwy kolejek
    COLLECTOR_QUEUE = "queue:collector"
    REFINERY_QUEUE = "queue:refinery"
    FINANCE_QUEUE = "queue:finance"
    
    def __init__(self, redis_client: Optional[RedisClient] = None):
        self.redis = redis_client or RedisClient()
    
    def send_to_refinery(self, task: dict) -> bool:
        """WyÅ›lij zadanie do przetworzenia przez AI"""
        return self.redis.publish_task(self.REFINERY_QUEUE, task)
    
    def send_to_finance(self, task: dict) -> bool:
        """WyÅ›lij paragon do weryfikacji"""
        return self.redis.publish_task(self.FINANCE_QUEUE, task)
    
    def get_stats(self) -> dict:
        """Statystyki wszystkich kolejek"""
        return {
            "collector": self.redis.get_queue_length(self.COLLECTOR_QUEUE),
            "refinery": self.redis.get_queue_length(self.REFINERY_QUEUE),
            "finance": self.redis.get_queue_length(self.FINANCE_QUEUE),
        }
```

### 6. ModuÅ‚ `shared/types.py` (Modele Pydantic)

```python
"""
WspÃ³lne typy danych dla wszystkich serwisÃ³w
"""
from pydantic import BaseModel, Field, HttpUrl
from typing import Optional, List, Dict, Any, Literal
from datetime import datetime
from enum import Enum


class TaskStatus(str, Enum):
    """Status przetwarzania zadania"""
    PENDING = "pending"
    PROCESSING = "processing"
    COMPLETED = "completed"
    FAILED = "failed"


class BaseTask(BaseModel):
    """Bazowa klasa dla wszystkich zadaÅ„"""
    id: str = Field(..., description="Unikalny identyfikator zadania")
    created_at: datetime = Field(default_factory=datetime.utcnow)
    status: TaskStatus = TaskStatus.PENDING
    
    class Config:
        use_enum_values = True


class ArticleTask(BaseTask):
    """Zadanie przetworzenia artykuÅ‚u web"""
    type: Literal["article"] = "article"
    url: HttpUrl
    title: Optional[str] = None
    content: str
    author: Optional[str] = None
    published_date: Optional[datetime] = None
    metadata: Dict[str, Any] = Field(default_factory=dict)
    
    class Config:
        json_schema_extra = {
            "example": {
                "id": "art_20250118_001",
                "type": "article",
                "url": "https://example.com/article",
                "title": "AI Trends 2025",
                "content": "Full article text...",
                "author": "John Doe",
            }
        }


class YoutubeTask(BaseTask):
    """Zadanie przetworzenia video YouTube"""
    type: Literal["youtube"] = "youtube"
    url: HttpUrl
    title: Optional[str] = None
    channel: Optional[str] = None
    transcript: str
    duration_seconds: Optional[int] = None
    thumbnail_url: Optional[str] = None
    metadata: Dict[str, Any] = Field(default_factory=dict)
    
    class Config:
        json_schema_extra = {
            "example": {
                "id": "yt_20250118_001",
                "type": "youtube",
                "url": "https://youtube.com/watch?v=xyz",
                "title": "ML Tutorial",
                "channel": "Tech Channel",
                "transcript": "Full transcript...",
                "duration_seconds": 1800,
            }
        }


class ReceiptTask(BaseTask):
    """Zadanie przetworzenia paragonu"""
    type: Literal["receipt"] = "receipt"
    image_path: str
    shop_name: Optional[str] = None
    purchase_date: Optional[datetime] = None
    total_amount: Optional[float] = None
    items: List[Dict[str, Any]] = Field(default_factory=list)
    ocr_raw_text: Optional[str] = None
    verified: bool = False
    
    class Config:
        json_schema_extra = {
            "example": {
                "id": "rec_20250118_001",
                "type": "receipt",
                "image_path": "/inbox/receipt_001.jpg",
                "shop_name": "Biedronka",
                "purchase_date": "2025-01-18T10:30:00",
                "total_amount": 45.67,
                "items": [
                    {"name": "Mleko", "price": 3.99, "quantity": 2},
                    {"name": "Chleb", "price": 2.50, "quantity": 1},
                ],
            }
        }


class ProcessedNote(BaseModel):
    """Wygenerowana notatka Markdown"""
    id: str
    title: str
    content: str  # PeÅ‚ny Markdown
    tags: List[str] = Field(default_factory=list)
    links: List[str] = Field(default_factory=list)  # Linki do innych notatek
    source_url: Optional[str] = None
    source_type: Literal["youtube", "article", "manual"]
    created_at: datetime = Field(default_factory=datetime.utcnow)
    vault_path: str  # Gdzie zapisaÄ‡ w Obsidian
    
    class Config:
        json_schema_extra = {
            "example": {
                "id": "note_20250118_001",
                "title": "AI Trends 2025",
                "content": "# AI Trends 2025\n\n## Summary\n...",
                "tags": ["AI", "trends", "2025"],
                "links": ["Machine Learning", "Neural Networks"],
                "source_url": "https://example.com/article",
                "source_type": "article",
                "vault_path": "Articles/2025-01/AI_Trends.md",
            }
        }


class ErrorResponse(BaseModel):
    """Standardowa odpowiedÅº bÅ‚Ä™du"""
    error: str
    details: Optional[str] = None
    task_id: Optional[str] = None
    timestamp: datetime = Field(default_factory=datetime.utcnow)
```

### 7. ModuÅ‚ `shared/config.py` (Konfiguracja)

```python
"""
Centralna konfiguracja dla wszystkich serwisÃ³w
"""
from pydantic_settings import BaseSettings, SettingsConfigDict
from functools import lru_cache


class Settings(BaseSettings):
    """
    Konfiguracja z zmiennych Å›rodowiskowych
    """
    
    # Redis
    redis_host: str = "redis"
    redis_port: int = 6379
    redis_db: int = 0
    
    # PostgreSQL
    postgres_user: str = "brain"
    postgres_password: str = "changeme"
    postgres_db: str = "obsidian_brain"
    postgres_host: str = "postgres"
    postgres_port: int = 5432
    
    # Ollama
    ollama_host: str = "http://ollama:11434"
    ollama_model: str = "deepseek-r1:14b"
    
    # API Keys (opcjonalne)
    openai_api_key: str = ""
    gemini_api_key: str = ""
    deepseek_api_key: str = ""
    
    # Paths
    obsidian_vault_path: str = "/vault"
    inbox_path: str = "/inbox"
    
    # Logging
    log_level: str = "INFO"
    log_format: str = "json"  # json | console
    
    model_config = SettingsConfigDict(
        env_file=".env",
        env_file_encoding="utf-8",
        case_sensitive=False,
    )
    
    @property
    def postgres_url(self) -> str:
        """Database URL for SQLAlchemy"""
        return (
            f"postgresql://{self.postgres_user}:{self.postgres_password}"
            f"@{self.postgres_host}:{self.postgres_port}/{self.postgres_db}"
        )


@lru_cache
def get_settings() -> Settings:
    """
    Singleton settings (cache)
    """
    return Settings()
```

### 8. ModuÅ‚ `shared/logging.py` (Strukturalne logowanie)

```python
"""
Strukturalne logowanie dla wszystkich serwisÃ³w
"""
import structlog
import logging
import sys
from typing import Optional


def setup_logging(
    level: str = "INFO",
    format: str = "json",
    service_name: Optional[str] = None
) -> None:
    """
    Konfiguracja structlog dla caÅ‚ego serwisu
    
    Args:
        level: DEBUG, INFO, WARNING, ERROR
        format: 'json' lub 'console'
        service_name: Nazwa serwisu (np. 'collector')
    """
    
    # Procesory wspÃ³lne dla wszystkich formatÃ³w
    shared_processors = [
        structlog.contextvars.merge_contextvars,
        structlog.stdlib.add_log_level,
        structlog.stdlib.add_logger_name,
        structlog.processors.TimeStamper(fmt="iso"),
        structlog.processors.StackInfoRenderer(),
    ]
    
    if service_name:
        shared_processors.append(
            structlog.processors.CallsiteParameterAdder(
                parameters=[
                    structlog.processors.CallsiteParameter.FILENAME,
                    structlog.processors.CallsiteParameter.LINENO,
                ]
            )
        )
    
    # WybÃ³r formatu
    if format == "json":
        processors = shared_processors + [
            structlog.processors.dict_tracebacks,
            structlog.processors.JSONRenderer(),
        ]
    else:  # console
        processors = shared_processors + [
            structlog.dev.ConsoleRenderer(colors=True),
        ]
    
    # Konfiguracja structlog
    structlog.configure(
        processors=processors,
        wrapper_class=structlog.stdlib.BoundLogger,
        context_class=dict,
        logger_factory=structlog.stdlib.LoggerFactory(),
        cache_logger_on_first_use=True,
    )
    
    # Konfiguracja standardowego loggera
    logging.basicConfig(
        format="%(message)s",
        stream=sys.stdout,
        level=getattr(logging, level.upper()),
    )
    
    # Dodaj service_name do kontekstu
    if service_name:
        structlog.contextvars.bind_contextvars(service=service_name)


def get_logger(name: Optional[str] = None) -> structlog.stdlib.BoundLogger:
    """
    Pobierz logger dla moduÅ‚u
    
    Args:
        name: Nazwa loggera (domyÅ›lnie __name__ wywoÅ‚ujÄ…cego)
    """
    return structlog.get_logger(name)
```

### 9. ModuÅ‚ `shared/utils.py` (Pomocnicze funkcje)

```python
"""
NarzÄ™dzia pomocnicze uÅ¼ywane w wielu miejscach
"""
import hashlib
from datetime import datetime
from pathlib import Path
from typing import Optional


def generate_task_id(prefix: str = "task") -> str:
    """
    Generuj unikalny ID zadania
    
    Args:
        prefix: Prefiks (np. 'yt', 'art', 'rec')
    
    Returns:
        ID w formacie: prefix_YYYYMMDD_HHMMSS_hash
    """
    timestamp = datetime.utcnow().strftime("%Y%m%d_%H%M%S")
    random_hash = hashlib.md5(str(datetime.utcnow()).encode()).hexdigest()[:6]
    return f"{prefix}_{timestamp}_{random_hash}"


def sanitize_filename(text: str, max_length: int = 100) -> str:
    """
    PrzeksztaÅ‚Ä‡ tekst na bezpiecznÄ… nazwÄ™ pliku
    
    Args:
        text: Oryginalny tekst
        max_length: Maksymalna dÅ‚ugoÅ›Ä‡ nazwy
    
    Returns:
        Bezpieczna nazwa pliku
    """
    # UsuÅ„ niebezpieczne znaki
    safe = "".join(c for c in text if c.isalnum() or c in (' ', '-', '_'))
    # ZamieÅ„ spacje na podkreÅ›lenia
    safe = safe.replace(' ', '_')
    # Ogranicz dÅ‚ugoÅ›Ä‡
    return safe[:max_length].strip('_')


def ensure_path_exists(path: Path) -> Path:
    """
    Upewnij siÄ™ Å¼e katalog istnieje (utwÃ³rz jeÅ›li nie)
    
    Args:
        path: ÅšcieÅ¼ka do katalogu
    
    Returns:
        Path object
    """
    path.mkdir(parents=True, exist_ok=True)
    return path


def calculate_file_hash(file_path: Path) -> str:
    """
    Oblicz MD5 hash pliku (do deduplikacji)
    
    Args:
        file_path: ÅšcieÅ¼ka do pliku
    
    Returns:
        MD5 hash jako hex string
    """
    hash_md5 = hashlib.md5()
    with open(file_path, "rb") as f:
        for chunk in iter(lambda: f.read(4096), b""):
            hash_md5.update(chunk)
    return hash_md5.hexdigest()
```

### 10. Testy `tests/test_messaging.py`

```python
"""
Testy dla moduÅ‚u messaging
"""
import pytest
from shared.messaging import RedisClient, TaskQueue
from shared.types import ArticleTask


@pytest.fixture
def redis_client():
    """Fixture z klientem Redis (wymaga dziaÅ‚ajÄ…cego Redis)"""
    client = RedisClient(host="localhost")
    yield client
    # Cleanup
    client.clear_queue("test_queue")


def test_redis_connection(redis_client):
    """Test poÅ‚Ä…czenia z Redis"""
    assert redis_client.ping() is True


def test_publish_task(redis_client):
    """Test publikacji zadania"""
    task = {"id": "test_001", "type": "test", "data": "hello"}
    result = redis_client.publish_task("test_queue", task)
    assert result is True
    assert redis_client.get_queue_length("test_queue") == 1


def test_listen_to_queue(redis_client):
    """Test nasÅ‚uchiwania na kolejce"""
    received_tasks = []
    
    def callback(task):
        received_tasks.append(task)
    
    # WyÅ›lij zadanie
    task = {"id": "test_002", "data": "test"}
    redis_client.publish_task("test_queue", task)
    
    # NasÅ‚uchuj (timeout aby nie czekaÄ‡ w nieskoÅ„czonoÅ›Ä‡)
    result = redis_client.client.brpop("test_queue", timeout=1)
    assert result is not None


def test_task_queue_wrapper():
    """Test wysokopoziomowego API"""
    queue = TaskQueue()
    
    task = ArticleTask(
        id="art_001",
        url="https://example.com",
        content="Test content"
    )
    
    result = queue.send_to_refinery(task.model_dump())
    assert result is True
```

### 11. Plik `README.md` dla biblioteki

```markdown
# Obsidian Brain Shared Library

WspÃ³lna biblioteka Python dla wszystkich mikroserwisÃ³w systemu Obsidian Brain v2.

## Instalacja

```bash
# W gÅ‚Ã³wnym katalogu projektu
cd shared
pip install -e .

# Lub z poziomu mikroserwisu
pip install -e ../shared
```

## UÅ¼ycie

### Messaging (Redis)

```python
from shared.messaging import RedisClient, TaskQueue
from shared.types import YoutubeTask

# Niskopoziomowy client
client = RedisClient(host="redis", port=6379)
client.publish_task("queue:refinery", {"id": "001", "type": "youtube"})

# Wysokopoziomowy wrapper
queue = TaskQueue()
task = YoutubeTask(
    id="yt_001",
    url="https://youtube.com/watch?v=xyz",
    transcript="Full transcript..."
)
queue.send_to_refinery(task.model_dump())
```

### Typy Danych

```python
from shared.types import ArticleTask, ProcessedNote

# Walidacja danych
task = ArticleTask(
    id="art_001",
    url="https://example.com/article",
    content="Article text...",
    title="Example Article"
)

# Export do JSON
json_data = task.model_dump_json()
```

### Konfiguracja

```python
from shared.config import get_settings

settings = get_settings()
print(settings.redis_host)  # redis
print(settings.postgres_url)  # postgresql://brain:pass@postgres/db
```

### Logowanie

```python
from shared.logging import setup_logging, get_logger

# W main.py serwisu
setup_logging(level="INFO", format="json", service_name="collector")

# W moduÅ‚ach
logger = get_logger(__name__)
logger.info("task_started", task_id="001", status="processing")
logger.error("task_failed", task_id="001", error="Connection timeout")
```

## Testy

```bash
# Zainstaluj zaleÅ¼noÅ›ci dev
pip install -e ".[dev]"

# Uruchom testy (wymaga dziaÅ‚ajÄ…cego Redis)
pytest tests/ -v

# Coverage
pytest tests/ --cov=shared --cov-report=html
```

## Dodawanie Nowych TypÃ³w

1. Dodaj model Pydantic w `types.py`
2. Zaktualizuj `__init__.py` (dodaj do `__all__`)
3. Dodaj testy w `tests/test_types.py`
4. Zaktualizuj dokumentacjÄ™

## Changelog

### v2.0.0 (2025-01-18)
- Inicjalna wersja dla architektury mikroserwisowej
- Redis messaging
- Pydantic models
- Structured logging
```

## ğŸ¯ Kryteria Sukcesu

### Walidacja po wykonaniu:

```bash
# 1. Instalacja pakietu dziaÅ‚a
cd shared
pip install -e .
# Expected: Successfully installed obsidian-brain-shared

# 2. Import dziaÅ‚a
python -c "from shared.messaging import RedisClient; print('OK')"
# Expected: OK

# 3. PoÅ‚Ä…czenie z Redis dziaÅ‚a
python -c "from shared.messaging import RedisClient; c = RedisClient(); print(c.ping())"
# Expected: True

# 4. Testy przechodzÄ…
pytest tests/ -v
# Expected: All tests passed

# 5. Typy dziaÅ‚ajÄ…
python -c "from shared.types import ArticleTask; t = ArticleTask(id='test', url='https://example.com', content='test'); print(t.model_dump_json())"
# Expected: Valid JSON output
```

### Checklist koÅ„cowy:

- [x] Struktura katalogÃ³w zgodna z zadaniem
- [x] `pip install -e ./shared` dziaÅ‚a bez bÅ‚Ä™dÃ³w
- [x] Wszystkie moduÅ‚y importujÄ… siÄ™ poprawnie
- [x] RedisClient Å‚Ä…czy siÄ™ z Redis z Agenta 1
- [x] Testy jednostkowe przechodzÄ…
- [x] Dokumentacja README.md kompletna
- [x] Pydantic models walidujÄ… dane poprawnie

## ğŸ“¦ Pliki WyjÅ›ciowe

```
shared/
â”œâ”€â”€ setup.py                âœ…
â”œâ”€â”€ requirements.txt        âœ…
â”œâ”€â”€ README.md              âœ…
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ __init__.py        âœ…
â”‚   â”œâ”€â”€ test_messaging.py  âœ…
â”‚   â”œâ”€â”€ test_types.py      âœ…
â”‚   â””â”€â”€ test_config.py     âœ…
â””â”€â”€ shared/
    â”œâ”€â”€ __init__.py        âœ…
    â”œâ”€â”€ messaging.py       âœ…
    â”œâ”€â”€ types.py          âœ…
    â”œâ”€â”€ config.py         âœ…
    â”œâ”€â”€ logging.py        âœ…
    â””â”€â”€ utils.py          âœ…
```

## ğŸ”— ZaleÅ¼noÅ›ci

**Wymaga:**
- âœ… Agent 1 (Infrastructure) - dziaÅ‚ajÄ…cy Redis

**Wymagane przez:**
- âœ… Agent 3 (Collector) - uÅ¼ywa messaging, types
- âœ… Agent 4 (Refinery) - uÅ¼ywa messaging, types, logging
- âœ… Agent 5 (Finance) - uÅ¼ywa types, config, logging

## ğŸ’¡ WskazÃ³wki dla Google Antigravity

### Testowanie podczas rozwoju:

```bash
# Terminal 1 - Redis (z Agenta 1)
docker compose up redis

# Terminal 2 - Python REPL do testÃ³w
cd shared
python
>>> from shared.messaging import RedisClient
>>> c = RedisClient()
>>> c.publish_task("test", {"hello": "world"})
```

### MoÅ¼liwe problemy:

**Redis niedostÄ™pny:**
- SprawdÅº czy Agent 1 uruchomiÅ‚ kontenery
- ZmieÅ„ host na `localhost` jeÅ›li testujesz poza Dockerem

**Import errors:**
- Upewnij siÄ™ Å¼e `pip install -e .` zostaÅ‚o uruchomione
- SprawdÅº czy jesteÅ› w virtualenv

**Pydantic validation errors:**
- SprawdÅº przykÅ‚ady w `json_schema_extra`
- UÅ¼yj `model_validate()` zamiast `__init__` dla debugowania

---

**Status:** ğŸŸ¢ Gotowy do uruchomienia
**Czas wykonania:** ~30 minut
**NastÄ™pny agent:** Agent 3, 4, lub 5 (moÅ¼na rÃ³wnolegle)
